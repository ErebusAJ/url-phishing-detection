{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "-hGSuUi_K2l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ukm0sgbsK0cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Utility Classes**"
      ],
      "metadata": {
        "id": "4O0XtCMfLKOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Wrapper\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# Clean DNN Model\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=[256, 128, 64], dropout=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden\n",
        "\n",
        "        for i in range(len(hidden)):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            layers.append(nn.BatchNorm1d(dims[i+1]))\n",
        "            layers.append(nn.LeakyReLU(0.1))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        layers.append(nn.Linear(dims[-1], 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "P4wq59lkLOpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load and Preprocess**"
      ],
      "metadata": {
        "id": "0X_Gog5NLX7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(csv_path, label_col=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"Loaded:\", df.shape)\n",
        "\n",
        "    # Auto-detect label column\n",
        "    possible_labels = [\"label\", \"class\", \"target\", \"Result\", \"is_phishing\"]\n",
        "    if label_col is None:\n",
        "        for c in possible_labels:\n",
        "            if c in df.columns:\n",
        "                label_col = c\n",
        "                break\n",
        "        if label_col is None:\n",
        "            label_col = df.columns[-1]\n",
        "\n",
        "    # Convert labels to 0/1\n",
        "    y = df[label_col]\n",
        "    def convert(v):\n",
        "        v = str(v).lower()\n",
        "        if v in [\"1\", \"true\", \"phishing\", \"malicious\", \"-1\"]:\n",
        "            return 1\n",
        "        return 0\n",
        "    y = y.apply(convert).values\n",
        "\n",
        "    X = df.drop(columns=[label_col]).copy()\n",
        "\n",
        "    # Remove useless columns\n",
        "    drop_cols = []\n",
        "    for c in X.columns:\n",
        "        if X[c].nunique() <= 1:\n",
        "            drop_cols.append(c)\n",
        "        if X[c].isna().mean() > 0.9:\n",
        "            drop_cols.append(c)\n",
        "    X.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    # Fill numeric missing\n",
        "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    for c in numeric_cols:\n",
        "        X[c] = X[c].fillna(X[c].median())\n",
        "\n",
        "    # Categorical handling\n",
        "    cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    small_cat = [c for c in cat_cols if X[c].nunique() <= 20]\n",
        "\n",
        "    ohe = None\n",
        "    if small_cat:\n",
        "        X[small_cat] = X[small_cat].fillna(\"MISSING\")\n",
        "        ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
        "        ohe_values = ohe.fit_transform(X[small_cat])\n",
        "        ohe_df = pd.DataFrame(ohe_values, index=X.index, columns=ohe.get_feature_names_out(small_cat))\n",
        "        X = pd.concat([X.drop(columns=small_cat), ohe_df], axis=1)\n",
        "\n",
        "    # Large categorical â†’ frequency encoding\n",
        "    large_cat = [c for c in cat_cols if c not in small_cat]\n",
        "    for c in large_cat:\n",
        "        freq = X[c].value_counts(normalize=True).to_dict()\n",
        "        X[c] = X[c].map(freq).fillna(0)\n",
        "\n",
        "    # Final numeric matrix\n",
        "    X = X.values.astype(np.float32)\n",
        "\n",
        "    # Outlier removal\n",
        "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "    mask = iso.fit_predict(X) == 1\n",
        "    X, y = X[mask], y[mask]\n",
        "\n",
        "    print(\"After Cleaning:\", X.shape)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "IKP7LugVLfgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Data**"
      ],
      "metadata": {
        "id": "A9sVqUoiLgxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"data/phishing.csv\"   # change this to your CSV\n",
        "X, y = load_and_preprocess(csv_path)"
      ],
      "metadata": {
        "id": "sORKMllTLkE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train/Val/Test Split**"
      ],
      "metadata": {
        "id": "kfOsEjVbLlk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified split\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, test_idx = next(sss.split(X, y))\n",
        "\n",
        "X_train, X_test = X[train_idx], X[test_idx]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "# Validation 10% of whole dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.11, stratify=y_train, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Val:\", X_val.shape)\n",
        "print(\"Test:\", X_test.shape)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "2k1wSsT7LsbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the DNN**"
      ],
      "metadata": {
        "id": "AEoF96P5LwEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "model = DNN(input_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_loader = DataLoader(TabularDataset(X_train, y_train), batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(TabularDataset(X_val, y_val), batch_size=256)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "patience = 10\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        best_state = model.state_dict()\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_state)"
      ],
      "metadata": {
        "id": "ATMPX0w5LzcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "CfKq0DKxL1sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(torch.from_numpy(X_test).float().to(device))\n",
        "    probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test, preds)\n",
        "prec = precision_score(y_test, preds)\n",
        "rec = recall_score(y_test, preds)\n",
        "f1 = f1_score(y_test, preds)\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"F1:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "hHWuD3ShL350"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}